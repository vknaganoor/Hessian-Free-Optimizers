{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "data_x = [[-1.,-2.,1.],[-2.,-3.,1.],[-3,-4.,1.],[1.,2.,1.],[2.,3.,1,],[3.,4.,1.]]\n",
    "data_y = [[1.,0.],[1.,0.],[1.,0.],[0.,1.],[0.,1.],[0.,1.]]\n",
    "\n",
    "batch_size = len(data_x);\n",
    "x = tf.placeholder(shape=[batch_size,3], dtype=tf.float32, name=\"x\")\n",
    "y = tf.placeholder(shape=[batch_size,2], dtype=tf.float32, name=\"y\")\n",
    "\n",
    "wrt_variables = tf.Variable(tf.ones(shape=[6,1]), dtype=tf.float32, name=\"wrt\")\n",
    "z = tf.Variable(tf.ones(shape=[6,1]), dtype=tf.float32, name=\"z\")\n",
    "wrt_variables_list = tf.unstack(wrt_variables)\n",
    "pred = tf.nn.softmax(tf.matmul(x, tf.reshape(wrt_variables_list,[3,2])))\n",
    "\n",
    "loss = -tf.reduce_sum(y*tf.log(pred),reduction_indices=[1])\n",
    "loss = tf.reshape(loss,[6,1])\n",
    "G = tf.gradients(loss,wrt_variables_list)\n",
    "temp2 = grad\n",
    "cg_wrt_variables = z\n",
    "\n",
    "for iteration in range(1):\n",
    "    cg_wrt_variables_list = tf.unstack(cg_wrt_variables)\n",
    "    R_op = Rop(loss,wrt_variables,cg_wrt_variables)\n",
    "    L_op = tf.transpose(Lop(loss,wrt_variables_list,R_op))\n",
    "    cg_loss = (G-L_op)**2\n",
    "    \n",
    "    [H,G] = Hess_Comp(cg_loss,cg_wrt_variables_list);\n",
    "    d=-G;\n",
    "    for index in range (0,len(cg_wrt_variables_list)-1):\n",
    "        Alpha_N= tf.matmul(-tf.transpose(G),d);\n",
    "        Alpha_D= tf.matmul(tf.matmul(tf.transpose(d),H),d);\n",
    "        Alpha = tf.divide(Alpha_N,Alpha_D);\n",
    "        cg_update_directions = Alpha*d    \n",
    "\n",
    "        cg_wrt_variables = tf.assign_add(cg_wrt_variables,cg_update_directions)\n",
    "        temp = Rop(loss,wrt_variables_list,cg_wrt_variables_list)\n",
    "\n",
    "        cg_wrt_variables_list = tf.unstack(cg_wrt_variables);\n",
    "        R_op = Rop(loss,wrt_variables,cg_wrt_variables)\n",
    "        L_op = tf.transpose(Lop(loss,wrt_variables_list,R_op))\n",
    "        cg_loss = (grad-L_op)**2\n",
    "\n",
    "        [H1,G]=Hess_Comp(cg_loss,cg_wrt_variables_list)\n",
    "        Beta_N= tf.matmul(tf.matmul(tf.transpose(G),H),d);\n",
    "        Beta_D= tf.matmul(tf.matmul(tf.transpose(d),H),d);\n",
    "        Beta = tf.divide(Beta_N,Beta_D);\n",
    "        Beta_update_directions = tf.unstack(Beta*d)\n",
    "        d=tf.subtract(Beta_update_directions,G)\n",
    "        [H,G]=Hess_Comp(cg_loss,cg_wrt_variables_list)\n",
    "update_directions = -cg_wrt_variables;\n",
    "update_directions=(tf.unstack(update_directions))\n",
    "\n",
    "wrt_variables = tf.assign_add(wrt_variables,update_directions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaco_comp(y,x_list):\n",
    "    Jaco=[];Jaco1=[];\n",
    "    Jaco.append([tf.gradients(y_, x_list) for y_ in y])\n",
    "    for i in range(0,len(Jaco[0])):\n",
    "        for j in range(0,len(Jaco[0][i])):\n",
    "            if Jaco[0][i][j] == None:\n",
    "                Jaco1.append(cons(0))   \n",
    "            else: \n",
    "                Jaco1.append(Jaco[0][i][j])\n",
    "    Jaco1 = tf.reshape(Jaco1,[len(y),len(x_list)])\n",
    "    return(Jaco1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Hess_Comp(y,x_list):\n",
    "    grads=[];H=[];grads1=[];H1=[];\n",
    "    grads.append(tf.gradients(y, x_list))\n",
    "    for i in range(0,len(grads[0])):\n",
    "        if grads[0][i] == None:\n",
    "            grads1.append(cons(10))   \n",
    "        else: \n",
    "            grads1.append(grads[0][i])\n",
    "    H.append([tf.gradients(y_, x_list) for y_ in grads1])\n",
    "    for i in range(0,len(H[0])):\n",
    "        for j in range(0,len(H[0][i])):\n",
    "            if H[0][i][j] == None:\n",
    "                H1.append(cons(10))   \n",
    "            else: \n",
    "                H1.append(H[0][i][j])\n",
    "    H1 = tf.reshape(H1,[len(x_list),len(x_list)])\n",
    "    grads1 = tf.reshape(grads1,[len(x_list),1])\n",
    "\n",
    "    return(H1,grads1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cons(x):\n",
    "    return tf.constant(x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "Grad: [array([ 6.], dtype=float32), array([-6.], dtype=float32), array([ 9.], dtype=float32), array([-9.], dtype=float32), array([ 0.], dtype=float32), array([ 0.], dtype=float32)]\n",
      "iteration 1: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 2: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 3: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 4: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 5: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 6: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 7: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 8: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 9: [ nan]\n",
      "Grad: [array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32), array([ nan], dtype=float32)]\n",
      "iteration 10: [ nan]\n",
      "Wrt_var: [[ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]\n",
      " [ nan]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suraj\\AppData\\Local\\Continuum\\anaconda2\\envs\\SurajKiranRaman\\lib\\site-packages\\ipykernel_launcher.py:34: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [[ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]\n",
      " [ nan  nan]]\n",
      "Expected: [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "print(\"Hi\")\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "initial_loss = sess.run(\n",
    "    loss,\n",
    "    feed_dict={\n",
    "        x: data_x,\n",
    "        y: data_y\n",
    "    }\n",
    ")\n",
    "\n",
    "for iteration in range(10):\n",
    "    g1,g,_,q = sess.run(\n",
    "        [wrt_variables,cg_loss,R_op,temp2],\n",
    "        feed_dict={\n",
    "            x: data_x,\n",
    "            y: data_y\n",
    "        }\n",
    "    )\n",
    "    print(\"Grad:\",q)    \n",
    "\n",
    "    comp_loss = sess.run(\n",
    "        loss,\n",
    "        feed_dict={\n",
    "            x: data_x,\n",
    "            y: data_y\n",
    "        }\n",
    "    )\n",
    "    print(\"iteration {}: {}\".format(iteration+1,sum(comp_loss)/len(data_x)))\n",
    "    if(sum(comp_loss)/len(data_x)<0.001):\n",
    "        break\n",
    "        \n",
    "print(\"Wrt_var:\",g1)\n",
    "print(\"Prediction:\", sess.run(pred, feed_dict={x: data_x}))\n",
    "print(\"Expected:\", data_y)\n",
    "\n",
    "sess.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients(f, x, grad_ys=None):\n",
    "    '''\n",
    "    An easier way of computing gradients in tensorflow. The difference from tf.gradients is\n",
    "        * If f is not connected with x in the graph, it will output 0s instead of Nones. This will be more meaningful\n",
    "            for computing higher-order gradients.\n",
    "        * The output will have the same shape and type as x. If x is a list, it will be a list. If x is a Tensor, it\n",
    "            will be a tensor as well.\n",
    "    :param f: A `Tensor` or a list of tensors to be differentiated\n",
    "    :param x: A `Tensor` or a list of tensors to be used for differentiation\n",
    "    :param grad_ys: Optional. It is a `Tensor` or a list of tensors having exactly the same shape and type as `f` and\n",
    "                    holds gradients computed for each of `f`.\n",
    "    :return: A `Tensor` or a list of tensors having the same shape and type as `x`\n",
    "    '''\n",
    "\n",
    "    if isinstance(x, list):\n",
    "        grad = tf.gradients(f, x, grad_ys=grad_ys)\n",
    "        for i in range(len(x)):\n",
    "            if grad[i] is None:\n",
    "                grad[i] = tf.zeros_like(x[i])\n",
    "        return grad\n",
    "    else:\n",
    "        grad = tf.gradients(f, x, grad_ys=grad_ys)[0]\n",
    "        if grad is None:\n",
    "            return tf.zeros_like(x)\n",
    "        else:\n",
    "            return grad\n",
    "\n",
    "\n",
    "def Lop(f, x, v):\n",
    "    '''\n",
    "    Compute Jacobian-vector product. The result is v^T @ J_x\n",
    "    :param f: A `Tensor` or a list of tensors for computing the Jacobian J_x\n",
    "    :param x: A `Tensor` or a list of tensors with respect to which the Jacobian is computed.\n",
    "    :param v: A `Tensor` or a list of tensors having the same shape and type as `f`\n",
    "    :return: A `Tensor` or a list of tensors having the same shape and type as `x`\n",
    "    '''\n",
    "    assert not isinstance(f, list) or isinstance(v, list), \"f and v should be of the same type\"\n",
    "    return gradients(f, x, grad_ys=v)\n",
    "\n",
    "\n",
    "def Rop(f, x, v):\n",
    "    '''\n",
    "    Compute Jacobian-vector product. The result is J_x @ v.\n",
    "    The method is inspired by [deep yearning's blog](https://j-towns.github.io/2017/06/12/A-new-trick.html)\n",
    "    :param f: A `Tensor` or a list of tensors for computing the Jacobian J_x\n",
    "    :param x: A `Tensor` or a list of tensors with respect to which the Jacobian is computed\n",
    "    :param v: A `Tensor` or a list of tensors having the same shape and type as `v`\n",
    "    :return: A `Tensor` or a list of tensors having the same shape and type as `f`\n",
    "    '''\n",
    "    assert not isinstance(x, list) or isinstance(v, list), \"x and v should be of the same type\"\n",
    "    if isinstance(f, list):\n",
    "        w = [tf.ones_like(_) for _ in f]\n",
    "    else:\n",
    "        w = tf.ones_like(f)\n",
    "    return (gradients(Lop(f, x, w), w, grad_ys=v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w =[[1, 1], [1, 1]]\n",
    "data_v = [[2, 2], [2, 2]]\n",
    "data_x = [[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_v = [[2, 2]]\n",
    "data_x = [[0,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(shape=[1,2], dtype=tf.float32, name=\"x\")\n",
    "V = tf.placeholder(shape=[2,2], dtype=tf.float32, name=\"V\")\n",
    "W = tf.placeholder(shape=[2,2], dtype=tf.float32, name=\"W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.matmul(x,W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROP_val = Rop(y, W, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "Rop_var = sess.run(\n",
    "    ROP_val,\n",
    "    feed_dict={\n",
    "        W: data_w,\n",
    "        V: data_v,\n",
    "        x: data_x\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  2.]]\n"
     ]
    }
   ],
   "source": [
    "print(Rop_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
